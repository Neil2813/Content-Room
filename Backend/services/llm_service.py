"""
LLM Service for ContentOS

AWS Bedrock-first with automatic fallback chain:
1. AWS Bedrock (Claude/Titan) - PRIMARY for hackathon
2. Grok (X.AI) - First fallback
3. Gemini (Google) - Second fallback (FREE: 60 QPM)
4. Ollama (Local) - Offline mode
5. Simple Templates - ULTIMATE fallback (no API needed)

Each provider is tried in order until one succeeds.
"""
import logging
import random
from abc import ABC, abstractmethod
from typing import Optional, Dict, Any, List
from enum import Enum

import httpx
from tenacity import retry, stop_after_attempt, wait_exponential

from config import settings

logger = logging.getLogger(__name__)


class LLMProvider(str, Enum):
    """Available LLM providers."""
    AWS_BEDROCK = "aws_bedrock"
    GROK = "grok"
    GEMINI = "gemini"
    OLLAMA = "ollama"
    SIMPLE = "simple_template"


class LLMError(Exception):
    """Base exception for LLM errors."""
    pass


class ProviderUnavailableError(LLMError):
    """Raised when a provider is unavailable."""
    pass


class AllProvidersFailedError(LLMError):
    """Raised when all providers fail."""
    pass


# ===========================================
# Provider Implementations
# ===========================================

class BaseLLMProvider(ABC):
    """Abstract base class for LLM providers."""
    
    @abstractmethod
    async def generate(self, prompt: str, **kwargs) -> str:
        """Generate text completion."""
        pass
    
    @abstractmethod
    def is_available(self) -> bool:
        """Check if provider is configured and available."""
        pass


class AWSBedrockProvider(BaseLLMProvider):
    """
    AWS Bedrock provider using Claude or Titan.
    PRIMARY for AWS hackathon.
    """
    
    def __init__(self):
        self.client = None
        if self.is_available():
            try:
                import boto3
                self.client = boto3.client(
                    'bedrock-runtime',
                    region_name=settings.aws_region,
                    aws_access_key_id=settings.aws_access_key_id,
                    aws_secret_access_key=settings.aws_secret_access_key,
                )
                logger.info("AWS Bedrock provider initialized")
            except Exception as e:
                logger.warning(f"Failed to initialize AWS Bedrock: {e}")
    
    def is_available(self) -> bool:
        return settings.aws_configured and settings.use_aws_bedrock
    
    async def generate(self, prompt: str, model: str = "anthropic.claude-3-sonnet-20240229-v1:0", **kwargs) -> str:
        if not self.client:
            raise ProviderUnavailableError("AWS Bedrock not configured")
        
        try:
            import json
            
            # Claude format for Bedrock
            body = json.dumps({
                "anthropic_version": "bedrock-2023-05-31",
                "max_tokens": kwargs.get("max_tokens", 1024),
                "messages": [{"role": "user", "content": prompt}]
            })
            
            response = self.client.invoke_model(
                modelId=model,
                body=body,
                contentType="application/json",
                accept="application/json"
            )
            
            result = json.loads(response['body'].read())
            return result['content'][0]['text']
            
        except Exception as e:
            logger.error(f"AWS Bedrock error: {e}")
            raise ProviderUnavailableError(f"AWS Bedrock failed: {e}")


class GrokProvider(BaseLLMProvider):
    """
    Groq provider using Groq Cloud API.
    Fast inference with open-source models.
    First fallback when AWS is unavailable.
    
    Note: Uses GROQ_API_KEY (gsk_*) from groq.com, not X.AI Grok.
    """
    
    def __init__(self):
        self.api_key = settings.grok_api_key  # Same env var, works for Groq
        self.base_url = "https://api.groq.com/openai/v1"  # Groq API endpoint
    
    def is_available(self) -> bool:
        return bool(self.api_key)
    
    async def generate(self, prompt: str, model: str = "llama-3.3-70b-versatile", **kwargs) -> str:
        if not self.api_key:
            raise ProviderUnavailableError("Groq API key not configured")
        
        try:
            async with httpx.AsyncClient(timeout=120.0) as client:
                response = await client.post(
                    f"{self.base_url}/chat/completions",
                    headers={
                        "Authorization": f"Bearer {self.api_key}",
                        "Content-Type": "application/json"
                    },
                    json={
                        "model": model,
                        "messages": [{"role": "user", "content": prompt}],
                        "max_tokens": kwargs.get("max_tokens", 1024),
                        "temperature": kwargs.get("temperature", 0.7),
                    }
                )
                response.raise_for_status()
                data = response.json()
                return data["choices"][0]["message"]["content"]
                
        except Exception as e:
            logger.error(f"Groq error: {e}")
            raise ProviderUnavailableError(f"Groq failed: {e}")


class GeminiProvider(BaseLLMProvider):
    """
    Google Gemini provider.
    FREE TIER: 60 requests/minute, 1M tokens/day.
    Second fallback.
    """
    
    def __init__(self):
        self.api_key = settings.gemini_api_key
        self.model = None
        if self.is_available():
            try:
                import google.generativeai as genai
                genai.configure(api_key=self.api_key)
                self.model = genai.GenerativeModel('gemini-1.5-flash')
                logger.info("Gemini provider initialized")
            except Exception as e:
                logger.warning(f"Failed to initialize Gemini: {e}")
    
    def is_available(self) -> bool:
        return bool(self.api_key)
    
    async def generate(self, prompt: str, **kwargs) -> str:
        if not self.model:
            raise ProviderUnavailableError("Gemini not configured")
        
        try:
            response = self.model.generate_content(prompt)
            return response.text
            
        except Exception as e:
            logger.error(f"Gemini error: {e}")
            raise ProviderUnavailableError(f"Gemini failed: {e}")


class OllamaProvider(BaseLLMProvider):
    """
    Ollama local provider.
    Completely FREE, runs locally.
    """
    
    def __init__(self):
        self.base_url = settings.ollama_base_url
        self.model = settings.ollama_model
        self._available = False
        self._check_availability()
    
    def _check_availability(self):
        """Check if Ollama is actually running."""
        try:
            import httpx
            response = httpx.get(f"{self.base_url}/api/tags", timeout=2.0)
            self._available = response.status_code == 200
        except:
            self._available = False
    
    def is_available(self) -> bool:
        return self._available
    
    async def generate(self, prompt: str, **kwargs) -> str:
        try:
            async with httpx.AsyncClient(timeout=120.0) as client:
                response = await client.post(
                    f"{self.base_url}/api/generate",
                    json={
                        "model": self.model,
                        "prompt": prompt,
                        "stream": False,
                    }
                )
                response.raise_for_status()
                data = response.json()
                return data["response"]
                
        except Exception as e:
            logger.error(f"Ollama error: {e}")
            raise ProviderUnavailableError(f"Ollama failed: {e}")


class SimpleTemplateProvider(BaseLLMProvider):
    """
    Simple template-based provider.
    ULTIMATE FALLBACK - no external API needed.
    Uses templates and randomization for demo purposes.
    """
    
    CAPTION_TEMPLATES = [
        "âœ¨ There's something magical about moments like these... {content} ðŸŒŸ What does this make you feel? Drop your thoughts below! ðŸ‘‡ #AestheticVibes #MoodBoard #Trending",
        "ðŸŒ¸ In a world of chaos, find your peace... {content} ï¿½ Tag someone who needs to see this beauty ðŸ’• #VibezOnly #BeautifulMoments #Aesthetic",
        "ï¿½ Stop scrolling, you need to see this! {content} âœ¨ Double tap if this hits different ï¿½ #InstaVibes #ContentCreator #Viral",
        "ï¿½ Life is all about these little wonders... {content} ðŸŒ¿ What's your favorite way to appreciate beauty? ðŸ’­ #SlowLiving #Mindful #NatureLovers",
        "â­ Some things just speak to the soul... {content} ðŸŽ¨ Save this for when you need a reminder â¤ï¸ #DailyInspiration #AestheticFeed #ContentRoom",
    ]
    
    SUMMARY_TEMPLATES = [
        "This content discusses {topic}. Key points include the main ideas presented.",
        "Summary: The content focuses on {topic} and provides valuable insights.",
        "In brief: {topic} is the central theme with important takeaways.",
    ]
    
    HASHTAGS = [
        "#ContentRoom", "#AI", "#ContentCreation", "#Digital", "#Tech",
        "#Innovation", "#Creative", "#Trending", "#Viral", "#Social",
        "#Marketing", "#Growth", "#Engagement", "#Strategy", "#Success",
    ]
    
    def is_available(self) -> bool:
        return True  # Always available
    
    async def generate(self, prompt: str, **kwargs) -> str:
        prompt_lower = prompt.lower()
        
        # Detect task type from prompt
        if "caption" in prompt_lower:
            # Extract content from prompt
            content = self._extract_content(prompt)
            
            # Determine target length from max_tokens kwarg
            max_tokens = kwargs.get("max_tokens", 256)
            target_chars = max_tokens * 3  # rough estimate
            
            template = random.choice(self.CAPTION_TEMPLATES)
            base_caption = template.format(content=content[:200])
            
            # If target is much larger than template, expand with more content
            if target_chars > 500 and len(base_caption) < target_chars:
                extras = [
                    "\n\nThis is one of those moments that truly stays with you. The kind of experience that makes you pause and appreciate what life has to offer.",
                    "\n\nThere's depth and beauty here that words can barely capture. Every detail tells its own story, and together they create something unforgettable.",
                    "\n\nWhat stands out most is how every element comes together so perfectly. It's the little things that make the biggest impact.",
                    "\n\nIf this resonated with you, share your own experience in the comments. Let's build a community of people who appreciate the extraordinary in the ordinary.",
                    "\n\nRemember: every great story starts with a moment just like this one. Don't let these moments pass you by without soaking them in.",
                ]
                while len(base_caption) < target_chars and extras:
                    base_caption += extras.pop(0)
            
            return base_caption[:target_chars] if target_chars < len(base_caption) else base_caption
        
        elif "summary" in prompt_lower or "summarize" in prompt_lower:
            content = self._extract_content(prompt)
            topic = content[:50] + "..." if len(content) > 50 else content
            template = random.choice(self.SUMMARY_TEMPLATES)
            return template.format(topic=topic)
        
        elif "hashtag" in prompt_lower:
            count = 5
            # Try to extract count from prompt
            for word in prompt.split():
                if word.isdigit():
                    count = int(word)
                    break
            selected = random.sample(self.HASHTAGS, min(count, len(self.HASHTAGS)))
            return "\n".join(selected)
        
        elif "rewrite" in prompt_lower or "tone" in prompt_lower:
            content = self._extract_content(prompt)
            if "professional" in prompt_lower:
                return f"We are pleased to inform you that {content}"
            elif "casual" in prompt_lower:
                return f"Hey! Just wanted to share - {content} ðŸ˜Š"
            elif "engaging" in prompt_lower:
                return f"ðŸ”¥ You won't believe this: {content}! ðŸš€"
            return content
        
        elif "moderation" in prompt_lower or "safety" in prompt_lower:
            # Return safe analysis
            return """SAFETY_SCORE: 85
FLAGS: none
EXPLANATION: Content appears to be safe for publication."""
        
        else:
            # Generic response
            return f"Generated response for: {prompt[:100]}..."
    
    def _extract_content(self, prompt: str) -> str:
        """Extract the main content from a prompt."""
        # Look for content after common markers
        markers = ["Content:", "content:", "Text:", "text:", "Original:"]
        for marker in markers:
            if marker in prompt:
                parts = prompt.split(marker)
                if len(parts) > 1:
                    # Get content until next section or end
                    content = parts[1].split("\n\n")[0].strip()
                    return content
        # Return last 200 chars if no marker found
        return prompt[-200:].strip()


# ===========================================
# Main LLM Service with Fallback Chain
# ===========================================

class LLMService:
    """
    LLM Service with automatic fallback chain.
    
    Priority: AWS Bedrock â†’ Grok â†’ Gemini â†’ Ollama â†’ Simple Templates
    """
    
    def __init__(self):
        self.providers: List[tuple[str, BaseLLMProvider]] = [
            (LLMProvider.AWS_BEDROCK, AWSBedrockProvider()),
            (LLMProvider.GROK, GrokProvider()),
            (LLMProvider.GEMINI, GeminiProvider()),
            (LLMProvider.OLLAMA, OllamaProvider()),
            (LLMProvider.SIMPLE, SimpleTemplateProvider()),  # Ultimate fallback
        ]
        
        # Log available providers
        available = [name for name, p in self.providers if p.is_available()]
        logger.info(f"LLM providers available: {available}")
    
    @retry(stop=stop_after_attempt(2), wait=wait_exponential(multiplier=1, min=1, max=10))
    async def generate(
        self,
        prompt: str,
        task: str = "general",
        **kwargs
    ) -> Dict[str, Any]:
        """
        Generate text using the fallback chain.
        
        Args:
            prompt: The input prompt
            task: Task type for logging
            **kwargs: Additional parameters (max_tokens, temperature, etc.)
        
        Returns:
            Dict with 'text', 'provider', and 'fallback_used'
        """
        errors = []
        fallback_used = False
        
        for i, (name, provider) in enumerate(self.providers):
            if not provider.is_available():
                continue
            
            try:
                logger.info(f"Trying LLM provider: {name} for task: {task}")
                text = await provider.generate(prompt, **kwargs)
                
                return {
                    "text": text,
                    "provider": name,
                    "fallback_used": fallback_used,
                }
                
            except ProviderUnavailableError as e:
                errors.append(f"{name}: {e}")
                logger.warning(f"Provider {name} failed, trying next...")
                fallback_used = True
                continue
        
        # All providers failed (shouldn't happen with SimpleTemplateProvider)
        error_msg = "; ".join(errors)
        logger.error(f"All LLM providers failed: {error_msg}")
        raise AllProvidersFailedError(f"All providers failed: {error_msg}")
    
    async def generate_caption(
        self, 
        content: str, 
        content_type: str = "text",
        max_length: int = 280,
        platform: Optional[str] = None
    ) -> Dict[str, Any]:
        """Generate a platform-optimized caption for content.
        
        Args:
            content: The content to generate a caption for
            content_type: Type of content (text, image, audio, video)
            max_length: Maximum caption length in characters (up to 3000)
            platform: Target platform (twitter, instagram, linkedin, custom)
        """
        # Clamp max_length to sensible bounds
        max_length = max(50, min(max_length, 3000))
        
        # Calculate appropriate max_tokens based on requested character length
        # Roughly 1 token â‰ˆ 3-4 characters; add generous padding
        estimated_tokens = max(256, int(max_length / 2.5) + 100)
        
        # Platform-specific tone customization
        if platform == "linkedin":
            tone_instructions = """- Tone: Professional, thought-leadership, industry-focused
- Style: Start with an insight or professional perspective
- Language: Clear, authoritative, and value-driven
- Emojis: Minimal (1-2 professional emojis like ðŸ’¼ ðŸ“Š ðŸš€)
- Hashtags: Industry-relevant and professional (#Leadership #Innovation #Business)
- Call-to-action: Invite professional discussion or connection"""
        elif platform == "twitter" or platform == "x":
            tone_instructions = """- Tone: Knowledgeable, Reserved, and Insightful
- Style: Concise, intelligent, and thought-provoking
- Language: Sharp, clear, intellectual without being pretentious
- Emojis: Very minimal (0-1 thoughtful emoji)
- Hashtags: Trending topics and knowledge-based tags
- Call-to-action: Spark intelligent conversation or retweets"""
        elif platform == "instagram":
            tone_instructions = """- Tone: Aesthetic, Dreamy, and Visually Evocative
- Style: Start with a mood-setting line or poetic phrase
- Language: Emotional, relatable, and visually descriptive
- Emojis: 3-5 aesthetic emojis spread throughout (âœ¨ ðŸŒ¸ ðŸ’« ðŸŒ™ ðŸ¦‹)
- Hashtags: Aesthetic and lifestyle tags (#AestheticVibes #InstaDaily #VisualMoodboard)
- Call-to-action: Engage emotions, tag friends, save for later"""
        else:
            # Default/Custom: balanced aesthetic approach
            tone_instructions = """- Tone: Engaging and relatable
- Style: Mix of aesthetic and informative
- Emojis: 2-4 relevant emojis
- Hashtags: Mix of trending and niche tags
- Call-to-action: Encourage engagement"""
        
        # Adjust length guidance based on the requested size
        if max_length >= 1500:
            length_guidance = f"""- You MUST write a LONG, detailed caption that is close to {max_length} characters.
- Write multiple paragraphs with rich detail, storytelling, and emotional depth.
- Include line breaks between paragraphs for readability.
- Aim for AT LEAST {int(max_length * 0.7)} characters. Going under {int(max_length * 0.5)} characters is UNACCEPTABLE."""
        elif max_length >= 500:
            length_guidance = f"""- Write a medium-length caption of approximately {max_length} characters.
- Include 2-3 paragraphs with good detail and engagement hooks.
- Aim for AT LEAST {int(max_length * 0.6)} characters."""
        else:
            length_guidance = f"""- Keep total length under {max_length} characters.
- Be concise but impactful."""
        
        prompt = f"""You are an expert social media copywriter creating a caption for {platform or 'social media'}.

Create a compelling caption for this {content_type} content.

Requirements:
- Target length: {max_length} characters
{length_guidance}
{tone_instructions}

Content: {content}

Write the caption now (no explanations, just the caption):"""
        return await self.generate(prompt, task="caption", max_tokens=estimated_tokens)
    
    async def generate_summary(self, content: str, max_length: int = 150) -> Dict[str, Any]:
        """Generate a summary of content.
        
        Args:
            content: The content to summarize
            max_length: Maximum summary length in characters
        """
        prompt = f"""Summarize the following content concisely.
Focus on the key points and main message.

IMPORTANT: Keep the summary under {max_length} characters.

Content: {content}

Summary:"""
        return await self.generate(prompt, task="summary")
    
    async def generate_hashtags(self, content: str, count: int = 5) -> Dict[str, Any]:
        """Generate hashtags for content."""
        prompt = f"""Generate {count} relevant hashtags for the following content.
Return only the hashtags, each on a new line, starting with #.
Make them trending-friendly and discoverable.

Content: {content}

Hashtags:"""
        result = await self.generate(prompt, task="hashtags")
        
        # Parse hashtags from response
        lines = result["text"].strip().split("\n")
        hashtags = [line.strip() for line in lines if line.strip().startswith("#")]
        
        # If no hashtags parsed, create from content
        if not hashtags:
            words = content.split()[:count]
            hashtags = [f"#{word.strip('.,!?').capitalize()}" for word in words if len(word) > 3]
        
        return {
            **result,
            "hashtags": hashtags[:count]
        }


# Singleton instance
_llm_service: Optional[LLMService] = None


def get_llm_service() -> LLMService:
    """Get or create the LLM service singleton."""
    global _llm_service
    if _llm_service is None:
        _llm_service = LLMService()
    return _llm_service
